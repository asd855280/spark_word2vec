{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment recipe text for fitting word2vec model, and save recipe_vector json file locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import jieba\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to run in cluster, which segment all chinese characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile('mydict_3.txt')\n",
    "def wordToSeg(x):\n",
    "    if not jieba.dt.initialized:\n",
    "        jieba.load_userdict('mydict_3.txt')\n",
    "    \n",
    "    try:\n",
    "        interstate = re.sub(r'\\W', '', x)\n",
    "    except:\n",
    "        interstate = x\n",
    "        pass\n",
    "    try:\n",
    "        secondstate = interstate.replace('\\n','')\n",
    "    except:\n",
    "        secondstate = interstate\n",
    "        pass\n",
    "    try:\n",
    "        thirdstate = secondstate.replace('\\n\\n','')\n",
    "    except:\n",
    "        thirdstate = secondstate\n",
    "        pass\n",
    "    try:\n",
    "        finalstate = re.sub(r'[a-zA-Z0-9]', '', thirdstate)\n",
    "    except:\n",
    "        finalstate = thirdstate\n",
    "        pass\n",
    "    try:\n",
    "        seg = jieba.cut(finalstate, cut_all = False)\n",
    "    except:\n",
    "        output = finalstate\n",
    "        pass\n",
    "    try:\n",
    "        output = ' '.join(seg)\n",
    "    except:\n",
    "        output = ''\n",
    "        pass\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"word2seg\", wordToSeg, StringType())\n",
    "word2Seg = udf(wordToSeg, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in all recipes information from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = spark.read.json(\"hdfs://master/user/spark/spark101/recipe_com/recipe_to_spark.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating temp view for following SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.createOrReplaceTempView(\"recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL query with pre-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply segment function on columns of ingredient, steps, comment and category\n",
    "recipes_seg = spark.sql('''select url, img_url, title, time, author, word2Seg(ingredient) ingredient, \n",
    "                        word2Seg(steps) steps, word2Seg(comment) comment,\n",
    "                        word2Seg(category) category from recipes''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_seg.createOrReplaceTempView(\"recipes_seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_wordbag = spark.sql('''SELECT concat(ingredient, steps, comment, category) as text from recipes_seg''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define another function for spliting words in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToList(x):\n",
    "    return x.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register UDF for spliting words to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"word2list\", wordToList, ArrayType(StringType()))\n",
    "word2list = udf(wordToList, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready to split the big string into list, then fit into word2vec training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_wordbag.createOrReplaceTempView(\"recipes_wordlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_word2vec = spark.sql('''SELECT word2list(text) text from recipes_wordlist''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLlib word2vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=50, minCount=3, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(for_word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"hdfs://master/user/spark/spark101/recipe_com/recipe_word2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform all recipes into vector by using the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_wordseg = spark.sql('''SELECT url, img_url, title, author, concat(ingredient, steps, comment, category) as text from recipes_seg''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_wordseg.createOrReplaceTempView(\"recipes_wordseg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_wordseglist = spark.sql('''SELECT url, img_url, title, author, word2list(text) as text from recipes_wordseg''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_vector = model.transform(recipes_wordseglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save recipe_vector to json on Hadoop hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_vector.write.json(\"hdfs://master/user/spark/spark101/recipe_com/recipe_vector.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can save recipe_vector json text file to local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_vector.coalesce(1).write.format('json').save(\"/home/spark/Desktop/recipe_com/recipe_vector.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the following command to download file from hdfs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -get /home/spark/Desktop/recipe_com/recipe_vector.json ~/Desktop/recipe_com/recipe_vector.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
